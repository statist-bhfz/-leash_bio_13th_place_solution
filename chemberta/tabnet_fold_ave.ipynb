{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c61623f-fc07-41bf-9d9e-28d4bb45defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import AveragePrecision\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    TQDMProgressBar,\n",
    ")\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel, DataCollatorWithPadding\n",
    "\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import polars as pl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b03357e-836b-4d45-ba3b-e6ede1f2ceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROTEIN_NAMES = [\"binds_BRD4\", \"binds_HSA\", \"binds_sEH\"]\n",
    "PROTEIN_NAMES = [\"BRD4\", \"HSA\", \"sEH\"]\n",
    "data_dir = Path(\"/tokenized-chemberta\")\n",
    "model_name = \"ChemBERTa-77M-MTR\"\n",
    "batch_size = 1024 #512\n",
    "\n",
    "trainer_params = {\n",
    "  \"max_epochs\": 10,\n",
    "  \"enable_progress_bar\": True,\n",
    "  \"accelerator\": \"auto\",\n",
    "  # \"precision\": \"16-mixed\",\n",
    "  \"precision\": \"16-mixed\",\n",
    "  \"gradient_clip_val\": None,\n",
    "  \"accumulate_grad_batches\": 6,\n",
    "  \"devices\": [0,1,2,3],\n",
    "  # 'strategy': 'ddp_spawn',\n",
    "}\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2174605-6061-4f20-9073-adc91a19c07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = np.array(df['input_ids'])\n",
    "        self.attention_masks = np.array(df['attention_mask'])\n",
    "        self.labels = np.array(df[PROTEIN_NAMES])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = {\n",
    "            \"input_ids\": torch.tensor(self.input_ids[index],dtype=torch.int32),\n",
    "            \"attention_mask\": torch.tensor(self.attention_masks[index],dtype=torch.bool),\n",
    "            \"labels\": torch.tensor(self.labels[index],dtype=torch.bool),  \n",
    "        }\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e891005-ce02-483a-8a6c-7acd829c2e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenize = pl.read_parquet(\n",
    "                                \"train_tokenized_77M-MTR_replaced_dy.parquet\"\n",
    "                                #  n_rows=10000\n",
    "                                 )\n",
    "train_dataset = CustomDataset(train_tokenize)\n",
    "del train_tokenize\n",
    "\n",
    "valid_tokenize = pl.read_parquet(\n",
    "                                'valid_tokenized_77M-MTR_replaced_dy.parquet',\n",
    "                                #  ,n_rows=10000\n",
    "                                 )\n",
    "valid_dataset = CustomDataset(valid_tokenize)\n",
    "del valid_tokenize\n",
    "gc.collect()\n",
    "\n",
    "all_data = pl.concat([train_tokenize, valid_tokenize])\n",
    "len_all_data = len(all_data)\n",
    "all_data_y = all_data[PROTEIN_NAMES].sum_horizontal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e51cee1-a5ce-4021-98ad-67ae92378bfc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(0.2619422201258426)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n",
    "\n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(0.2619422201258426)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.leaky_relu(self.dense1(x))\n",
    "\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.dense2(x))\n",
    "\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        y = self.dense3(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "class GBN(nn.Module):\n",
    "    def __init__(self,inp,vbs=128,momentum=0.01):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(inp,momentum=momentum)\n",
    "        self.vbs = vbs\n",
    "    def forward(self,x):\n",
    "        chunk = torch.chunk(x,max(1,x.size(0)//self.vbs),0)\n",
    "        res = [self.bn(y) for y in chunk ]\n",
    "        return torch.cat(res,0)\n",
    "\n",
    "class GLU(nn.Module):\n",
    "    def __init__(self,inp_dim,out_dim,fc=None,vbs=128):\n",
    "        super().__init__()\n",
    "        if fc:\n",
    "            self.fc = fc\n",
    "        else:\n",
    "            self.fc = nn.Linear(inp_dim,out_dim*2)\n",
    "        self.bn = GBN(out_dim*2,vbs=vbs)\n",
    "        self.od = out_dim\n",
    "        self.dropout = nn.Dropout(0.2619422201258426)\n",
    "    def forward(self,x):\n",
    "        x = self.dropout(self.bn(F.leaky_relu((self.fc(x)))))\n",
    "        return x[:,:self.od]*torch.sigmoid(x[:,self.od:])\n",
    "\n",
    "\n",
    "class FeatureTransformer(nn.Module):\n",
    "    def __init__(self,inp_dim,out_dim,shared,n_ind,vbs=128):\n",
    "        super().__init__()\n",
    "        first = True\n",
    "        self.shared = nn.ModuleList()\n",
    "        if shared:\n",
    "            self.shared.append(GLU(inp_dim,out_dim,shared[0],vbs=vbs))\n",
    "            first= False\n",
    "            for fc in shared[1:]:\n",
    "                self.shared.append(GLU(out_dim,out_dim,fc,vbs=vbs))\n",
    "        else:\n",
    "            self.shared = None\n",
    "        self.independ = nn.ModuleList()\n",
    "        if first:\n",
    "            if shared:\n",
    "                self.independ.append(GLU(inp_dim,out_dim,vbs=vbs))\n",
    "            else:\n",
    "                self.independ.append(GLU(out_dim,out_dim,vbs=vbs))\n",
    "        for x in range(first, n_ind):\n",
    "            self.independ.append(GLU(out_dim,out_dim,vbs=vbs))\n",
    "        self.scale = torch.sqrt(torch.tensor([.5])) #,device=device\n",
    "        self.dropout = nn.Dropout(0.2619422201258426)\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "        self.fc = nn.Linear(inp_dim,out_dim)\n",
    "    def forward(self,x):\n",
    "        if self.shared:\n",
    "            x = self.dropout(self.bn(F.leaky_relu(self.shared[0](x))))\n",
    "            for glu in self.shared[1:]:\n",
    "                glu_x = self.dropout(glu(x))\n",
    "                x = torch.add(x, glu_x)\n",
    "                x = x*self.scale\n",
    "        else:\n",
    "            x = self.dropout(self.bn(F.leaky_relu(self.fc(x))))\n",
    "        for glu in self.independ:\n",
    "            glu_x = self.dropout(glu(x))\n",
    "            x = torch.add(x, glu_x)\n",
    "            x = x*self.scale.to(x.device)\n",
    "        return x\n",
    "class AttentionTransformer(nn.Module):\n",
    "    def __init__(self,inp_dim,out_dim,relax,vbs=128):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(inp_dim,out_dim)\n",
    "        self.bn = GBN(out_dim,vbs=vbs)\n",
    "        self.r = torch.tensor([relax]) #,device=device\n",
    "    def forward(self,a,priors):\n",
    "        a = self.bn(self.fc(a))\n",
    "        mask = torch.sigmoid(a*priors)\n",
    "        priors =priors*(self.r.to(a.device)-mask)\n",
    "        return mask,priors\n",
    "\n",
    "class DecisionStep(nn.Module):\n",
    "    def __init__(self,inp_dim,n_d,n_a,shared,n_ind,relax,vbs=128):\n",
    "        super().__init__()\n",
    "        self.fea_tran = FeatureTransformer(inp_dim,n_d+n_a,shared,n_ind,vbs)\n",
    "        self.atten_tran = AttentionTransformer(n_a,inp_dim,relax,vbs)\n",
    "    def forward(self,x,a,priors):\n",
    "        mask,priors = self.atten_tran(a,priors)\n",
    "        loss = ((-1)*mask*torch.log(mask+1e-10)).mean()\n",
    "        x = self.fea_tran(x*mask)#x*mask\n",
    "        return x,loss,priors\n",
    "\n",
    "\n",
    "class TabNet(nn.Module):\n",
    "    def __init__(self,inp_dim,final_out_dim,n_d=64,n_a=64,n_shared=2,\n",
    "                 n_ind=2,n_steps=5,relax=1.2,vbs=128,model_name=\"ChemBERTa-77M-MTR\"):\n",
    "        super().__init__()\n",
    "        self.lm = AutoModel.from_pretrained(\"DeepChem/\"+model_name, add_pooling_layer=False)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "        if n_shared>0:\n",
    "            self.shared = nn.ModuleList()\n",
    "            self.shared.append(nn.Linear(inp_dim,2*(n_d+n_a)))\n",
    "            for x in range(n_shared-1):\n",
    "                self.shared.append(nn.Linear(n_d+n_a,2*(n_d+n_a)))\n",
    "        else:\n",
    "            self.shared=None\n",
    "        self.first_step = FeatureTransformer(inp_dim,n_d+n_a,self.shared,n_ind)\n",
    "        self.steps = nn.ModuleList()\n",
    "        for x in range(n_steps-1):\n",
    "            self.steps.append(DecisionStep(inp_dim,n_d,n_a,self.shared,n_ind,relax,vbs))\n",
    "        self.fc = Model(n_d,final_out_dim,500)\n",
    "        self.bn = nn.BatchNorm1d(inp_dim)\n",
    "        self.n_d = n_d\n",
    "    def forward(self,x):\n",
    "\n",
    "        output = self.lm(\n",
    "            x[\"input_ids\"],\n",
    "            attention_mask=x[\"attention_mask\"],\n",
    "        ).last_hidden_state\n",
    "        output = output[:, 0]\n",
    "        \n",
    "        output = self.bn(output)\n",
    "        \n",
    "        x_a = self.first_step(output)[:,self.n_d:]\n",
    "        loss = torch.zeros(1).to(output.device)\n",
    "        out = torch.zeros(output.size(0),self.n_d).to(output.device)\n",
    "        priors = torch.ones(output.shape).to(output.device)\n",
    "        for step in self.steps:\n",
    "            x_te,l,priors = step(output,x_a,priors)\n",
    "            out += F.relu(x_te[:,:self.n_d])\n",
    "            x_a = x_te[:,self.n_d:]\n",
    "            loss += l\n",
    "        # return self.fc(out)\n",
    "        output = self.fc(out)\n",
    "        return {\n",
    "                \"logits\": output\n",
    "            }\n",
    "        \n",
    "    def calculate_loss(self, x):\n",
    "        output = self.forward(x)\n",
    "        loss = self.loss_fn(output['logits'], x[\"labels\"].float())\n",
    "        output[\"loss\"] = loss\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ffcdb6-6af2-4278-9776-4fec0fa060e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LBModelModule(L.LightningModule):\n",
    "    def __init__(self, model_name, batch_size):\n",
    "        super().__init__()\n",
    "        self.model = TabNet(inp_dim=384,final_out_dim=3,model_name=model_name,n_d=128,n_a=256,n_shared=1,n_ind=1,n_steps=3,relax=2,vbs=128)\n",
    "        self.map = AveragePrecision(task=\"binary\")\n",
    "        self.map_per_class = [AveragePrecision(task=\"binary\") for _ in range(3)]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.model(batch)\n",
    "    def calculate_loss(self, batch, batch_idx):\n",
    "        return self.model.calculate_loss(batch)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        ret = self.calculate_loss(batch, batch_idx)\n",
    "        self.log(\"train_loss\", ret[\"loss\"], on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        return ret[\"loss\"]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        ret = self.calculate_loss(batch, batch_idx)\n",
    "        self.log(\"val_loss\", ret[\"loss\"], on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.map.update(F.sigmoid(ret[\"logits\"]), batch[\"labels\"].long())\n",
    "\n",
    "        for i in range(3):\n",
    "            self.map_per_class[i].update(F.sigmoid(ret[\"logits\"])[:, i], batch[\"labels\"].long()[:, i])\n",
    "    def on_validation_epoch_end(self):\n",
    "        val_map = self.map.compute()\n",
    "        self.log(\"val_map\", val_map, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        for i in range(3):\n",
    "            val_map = self.map_per_class[i].compute()\n",
    "            self.log(f\"val_map_{PROTEIN_NAMES[i]}\", val_map, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "            self.map_per_class[i].reset()\n",
    "        self.map.reset()\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        logits = self.forward(batch)[\"logits\"]\n",
    "        probs = F.sigmoid(logits)\n",
    "        return probs\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # Return your dataloader here\n",
    "        return DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True,num_workers=0,collate_fn=DataCollatorWithPadding(tokenizer))\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # Return your dataloader here\n",
    "        return DataLoader(valid_dataset, batch_size=self.batch_size, shuffle=False,num_workers=0, #4 pin_memory=True,\n",
    "                              collate_fn=DataCollatorWithPadding(tokenizer))\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience= 2, verbose=True)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50816f5-d58e-4c83-a291-9aa0ed9bb770",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/\"+model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5f449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "FOLD = [0,1,2,3,4]\n",
    "\n",
    "train_dataset = None\n",
    "valid_dataset = None\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(np.zeros(len_all_data), all_data_y)):\n",
    "    print(f'Fold {fold + 1}/{n_splits}')\n",
    "\n",
    "    if fold not in FOLD:\n",
    "        continue;\n",
    "    \n",
    "    all_data = pl.read_parquet(\"all_data.parquet\")\n",
    "\n",
    "    train = all_data[train_idx]\n",
    "    valid = all_data[val_idx][:10000]\n",
    "\n",
    "    del all_data\n",
    "    gc.collect()\n",
    "    \n",
    "    train_dataset = CustomDataset(train)\n",
    "    valid_dataset = CustomDataset(valid)\n",
    "    \n",
    "    del train, valid\n",
    "\n",
    "    modelmodule = LBModelModule(model_name, batch_size, train_dataset, valid_dataset)\n",
    "\n",
    "    EXP_NAME = f'5fold_chemberta_model3_fold{fold + 1}'\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        filename=f\"model_{model_name}_fold{fold + 1}_{{val_map:.4f}}\",\n",
    "        save_weights_only=True,\n",
    "        monitor=\"val_map\",\n",
    "        mode=\"max\",\n",
    "        dirpath=f\"chemberta_v3_5fold/fold{fold+1}\",\n",
    "        save_top_k=5,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_map\", mode=\"max\", patience=5)\n",
    "  \n",
    "    progress_bar_callback = TQDMProgressBar(refresh_rate=1)\n",
    "   \n",
    "    callbacks = [checkpoint_callback, early_stop_callback, progress_bar_callback]\n",
    "  \n",
    "    trainer = L.Trainer(callbacks=callbacks, **trainer_params)\n",
    "\n",
    "    trainer.fit(modelmodule) #, train_dataloader, valid_dataloader\n",
    "    del train_dataset, valid_dataset # , train_dataloader, valid_dataloader\n",
    "    # gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb20aa",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67fe0f47-d37b-48f6-a0d3-bf822987fba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = np.array(df['input_ids'])\n",
    "        self.attention_masks = np.array(df['attention_mask'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.attention_masks)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = {\n",
    "            \"input_ids\": torch.tensor(self.input_ids[index],dtype=torch.int32),\n",
    "            \"attention_mask\": torch.tensor(self.attention_masks[index],dtype=torch.bool),\n",
    "        }\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca41feb-278d-4cfe-9ae6-01bd0ddcee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(callbacks=callbacks, **trainer_params)\n",
    "EPOCHS = [1,2,3,4,5]\n",
    "# !mv /kaggle/input/leash-bio-model-weights/model_ChemBERTa-77M-MTR_fold1_epoch3.ckpt /kaggle/input/leash-bio-model-weights/chemberta_v3_5fold/fold1\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/\"+model_name)\n",
    "test_tokenize = pl.read_parquet(Path(data_dir, f'test_tokenized_77M_replace_dy.parquet'))\n",
    "test_dataset = CustomTestDataset(test_tokenize)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,pin_memory=True,num_workers=1,\n",
    "                              collate_fn=DataCollatorWithPadding(tokenizer))\n",
    "\n",
    "fold_predictions = []\n",
    "\n",
    "for EPOCH in EPOCHS:\n",
    "    fold_predictions = []\n",
    "    for FOLD in range(1,6):\n",
    "    #     model_path = Path(f'/kaggle/input/leash-bio-model-weights/chemberta_v3_5fold/fold{FOLD}/model_ChemBERTa-77M-MTR_fold{FOLD}_epoch{EPOCH}.ckpt')\n",
    "        model_path = f'model_ChemBERTa-77M-MTR_fold{FOLD}_epoch{EPOCH}.ckpt'\n",
    "        # /home/sato/kag/chemberta_output/model2/model_ChemBERTa-77M-MTR_val_map=0.4487.ckpt\n",
    "        print(model_path)\n",
    "        modelmodule = LBModelModule.load_from_checkpoint(\n",
    "            checkpoint_path=model_path,\n",
    "            model_name=model_name,\n",
    "        )\n",
    "\n",
    "        predictions = trainer.predict(modelmodule, test_dataloader)\n",
    "\n",
    "        predictions = torch.cat(predictions).numpy()\n",
    "        fold_predictions.append(predictions)\n",
    "    avg_predictions = sum(fold_predictions) / len(fold_predictions)\n",
    "\n",
    "\n",
    "    pred_dfs = []\n",
    "    for i, protein_name in enumerate(PROTEIN_NAMES):\n",
    "        pred_dfs.append(\n",
    "            test_tokenize.with_columns(\n",
    "                pl.lit(protein_name).alias(\"protein_name\"),\n",
    "                pl.lit(avg_predictions[:, i]).alias(\"binds\"),\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    pred_df = pl.concat(pred_dfs)\n",
    "\n",
    "    submit_df = (\n",
    "        pl.read_parquet(\"/kaggle/input/leash-BELKA/test.parquet\", columns=[\"id\", \"molecule_smiles\", \"protein_name\"])\n",
    "        .join(pred_df, on=[\"id\", \"protein_name\"], how=\"left\")\n",
    "        .select([\"id\", \"binds\"])\n",
    "        .sort(\"id\")\n",
    "    )\n",
    "    \n",
    "    submit_df.group_by('id').mean().write_csv(f\"chemberta_5fold_{EPOCH}.csv\")\n",
    "    print(submit_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
